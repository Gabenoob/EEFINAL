{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that is widely used in data analysis and machine learning. It transforms the data into a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on. This technique helps in reducing the number of variables while preserving as much information as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "\n",
    "SVD is a subset of PCA. It is a matrix factorization technique that factors a matrix into three matrices: U, Σ, and V. The SVD of a matrix A is given by:\n",
    "\n",
    "$$A = U \\Sigma V^T$$\n",
    "\n",
    "where:\n",
    "- U is an m x m orthogonal matrix.\n",
    "- Σ is an m x n diagonal matrix with non-negative real numbers on the diagonal.\n",
    "\n",
    "$A A^T$ and $A^T A$ are symmetric matrices, so they have an eigendecomposition:\n",
    "\n",
    "$$A A^T = U \\Sigma \\Sigma^T U^T$$\n",
    "\n",
    "$A^T A$ is a symmetric matrix, so it has an eigendecomposition:\n",
    "\n",
    "$$A^T A = V \\Sigma^T \\Sigma V^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.27521645  0.54830912  0.62935278  0.47699905]\n",
      " [ 0.73735103  0.40455249 -0.39371346 -0.37099926]\n",
      " [ 0.57730663 -0.70752362  0.4041267  -0.05299989]\n",
      " [ 0.21745439 -0.18736255 -0.53440284  0.79499841]]\n",
      "[[16.74544917  0.          0.        ]\n",
      " [ 0.          3.84952987  0.        ]\n",
      " [ 0.          0.          0.87809551]\n",
      " [ 0.          0.          0.        ]]\n",
      "[[ 0.45590838  0.88912003  0.04016359]\n",
      " [ 0.4004734  -0.16462997 -0.90139782]\n",
      " [ 0.79483873 -0.42703927  0.43112511]]\n",
      "[[4. 1. 3.]\n",
      " [7. 5. 9.]\n",
      " [2. 4. 9.]\n",
      " [1. 2. 3.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# SVD\n",
    "X = np.array([[4, 1, 3], [7, 5, 9], [2, 4, 9], [1, 2, 3]])\n",
    "XXT = np.dot(X, X.T)\n",
    "XTX = np.dot(X.T, X)\n",
    "ei_values, ei_vectors = np.linalg.eig(XXT)\n",
    "ei_values2, ei_vectors2 = np.linalg.eig(XTX)\n",
    "S = np.diag(np.sqrt(ei_values))\n",
    "S = S[: XXT.shape[0], : XTX.shape[0]]\n",
    "U = ei_vectors\n",
    "V = ei_vectors2\n",
    "print(U)\n",
    "print(S)\n",
    "print(V)\n",
    "print(np.dot(np.dot(U, S), V.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "Non-negative Matrix Factorization (NMF) is a dimensionality reduction technique that is widely used in data analysis and machine learning. It factorizes the given non-negative matrix into two non-negative matrices such that the product of these two matrices closely approximates the original matrix. This technique helps in reducing the number of variables while preserving as much information as possible.\n",
    "\n",
    "*The main advantage of NMF* compared to previously introduced low-rank models is that the nonnegativity constraints on the factors W and H lead to an easily interpretable part-based decomposition. In application like image processing and text mining.\n",
    "\n",
    "\n",
    "$$ X \\approx W H $$\n",
    "\n",
    "where:\n",
    "- $X$ is an m x n non-negative matrix.\n",
    "- $W$ is an m x r non-negative matrix.\n",
    "- $H$ is an r x n non-negative matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplex Structure Matrix Factorization (SSMF)\n",
    "\n",
    "SSMF is a generalization of NMF.\n",
    "\n",
    "$$ X \\approx W H $$\n",
    "\n",
    "where:\n",
    "- $X$ is an m x n non-negative matrix.\n",
    "- $W$ is an m x r non-negative matrix.\n",
    "- $H$ is column-stochastic matrix.\n",
    "$$\n",
    "H(:,j) \\in \\Delta^{r} for\\ all\\ j,\\\\\\Delta^{r} = \\{x \\in R^{r} | x \\geq 0, e^T x = 1\\}\\\\e = \\begin{bmatrix}1\\\\1\\\\\\vdots\\\\1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "*The main advantage of SSMF* is that the column-stochastic constraints on the factors H lead to an easier interpretation of the factors. The factors in H can be interpreted as probability distributions over the columns of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why SSMF is a generalization of NMF?\n",
    "\n",
    "1. Normalize the input matrix $X$ s.t. $X \\in \\Delta^{m \\times n}$\n",
    "$$ X^Te = e $$\n",
    "2. Normalize the factor matrix $W$ s.t. $W \\in \\Delta^{m \\times r}$\n",
    "$$ W^Te = e $$\n",
    "\n",
    "Then, we have:\n",
    "$$ X \\approx W H $$\n",
    "$$ X^Te = (W H)^Te = H^TW^Te = H^Te = e $$\n",
    "$$ H^Te = e $$\n",
    "$$ H(:,j) \\in \\Delta^{r} for\\ all\\ j $$\n",
    "\n",
    "$$ \\frac{X}{\\|X\\|_1} = \\frac{W}{\\|W\\|_1} \\frac{H \\cdot {\\|W\\|_1}^T}{\\|X\\|_1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71786685 0.13548403]\n",
      " [1.15088496 0.98347962]\n",
      " [0.11221287 1.4110256 ]\n",
      " [0.         0.93836125]]\n",
      "[[5.26575428 1.77185679 2.65873193]\n",
      " [1.00610417 2.62251602 6.19591016]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.91642151, 1.62726631, 2.74806243],\n",
       "       [7.04976036, 4.6183944 , 9.15344599],\n",
       "       [2.01052413, 3.89926239, 9.04093182],\n",
       "       [0.94408916, 2.4608674 , 5.81400198]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[4, 1, 3], [7, 5, 9], [2, 4, 9], [1, 2, 6]])\n",
    "# nmf\n",
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=2)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "print(W)\n",
    "print(H)\n",
    "np.dot(W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36238246 0.03906296]\n",
      " [0.58097197 0.28355831]\n",
      " [0.05664557 0.40682901]\n",
      " [0.         0.27054972]]\n",
      "[[0.74509095 0.29249881 0.19506867]\n",
      " [0.24925157 0.75798373 0.79591067]]\n",
      "[[0.28571429 0.08333333 0.11111111]\n",
      " [0.5        0.41666667 0.33333333]\n",
      " [0.14285714 0.33333333 0.33333333]\n",
      " [0.07142857 0.16666667 0.22222222]]\n",
      "[[0.99434251]\n",
      " [1.05048254]\n",
      " [0.99097934]]\n"
     ]
    }
   ],
   "source": [
    "# SSMF one way\n",
    "normX1 = np.sum(X, axis=0)\n",
    "normW1 = np.sum(W, axis=0)\n",
    "X_norm = X / normX1\n",
    "W_norm = W / normW1\n",
    "H_new = ((H / normX1).T * normW1).T\n",
    "print(W_norm)\n",
    "print(H_new)\n",
    "print(X_norm)\n",
    "\n",
    "print(np.dot(H_new.T, np.ones((2,1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essential Uniqueness\n",
    "\n",
    "## Definition\n",
    "Let $X \\in \\Re^{m \\times n}$, and $r \\leq min(m, n)$ be an integer. Let $(W,H)$ be a solution of $X = WH$ if and only if any other pair $(W',H') \\in \\Re^{m \\times r}\\times \\Re^{r\\times n}$ that satisfies $X = W'H'$ for all k, $$W'(:,k)=\\alpha _kW(:,\\pi (k))$$ and $$H'(k,:)=\\alpha _k^{-1}H(\\pi (k),:)$$ where $\\pi$ is a permutation of $\\{1,2,...,r\\}$ and $\\alpha _k \\neq 0$ for all k."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
